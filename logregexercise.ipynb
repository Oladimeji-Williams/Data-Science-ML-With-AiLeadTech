{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries and classes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset\n",
    "titanic = pd.read_csv(r\"C:\\Users\\Williams\\Desktop\\DataScienceWithAiLead\\data\\titanic.csv\")\n",
    "titanic_new = titanic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning:\n",
    "\n",
    "#keep rows with 70% of columns filled\n",
    "titanic_new = titanic_new.dropna(thresh=(len(titanic_new.columns)*0.7))\n",
    "\n",
    "#fill the nulls of \"fare\" column with the most occuring value\n",
    "titanic_new[\"fare\"] = titanic_new[\"fare\"].fillna(value=titanic_new[\"fare\"].mode()[0])\n",
    "\n",
    "#forward fill the nulls of \"embarked\" column\n",
    "titanic_new[\"embarked\"] = titanic_new[\"embarked\"].fillna(method=\"ffill\")\n",
    "\n",
    "#randomly fill the nulls of the rest of the columns\n",
    "titanic_new[\"age\"] = titanic_new[\"age\"].fillna(value=titanic_new[\"age\"].dropna().sample().to_string(index=False))\n",
    "titanic_new[\"age\"] = pd.to_numeric(titanic_new[\"age\"])\n",
    "titanic_new[\"cabin\"] = titanic_new[\"cabin\"].fillna(value=titanic_new[\"cabin\"].dropna().sample().to_string(index=False))\n",
    "titanic_new[\"boat\"] = titanic_new[\"boat\"].fillna(value=titanic_new[\"boat\"].dropna().sample().to_string(index=False))\n",
    "titanic_new[\"body\"] = titanic_new[\"body\"].fillna(value=titanic_new[\"body\"].dropna().sample().to_string(index=False))\n",
    "titanic_new[\"home.dest\"] = titanic_new[\"home.dest\"].fillna(value=titanic_new[\"home.dest\"].dropna().sample().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical features\n",
    "sex = pd.get_dummies(titanic_new[\"sex\"], drop_first=True)\n",
    "embarked = pd.get_dummies(titanic_new[\"embarked\"], drop_first=True)\n",
    "titanic_new = pd.concat([titanic_new, sex, embarked], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select features and target\n",
    "X = titanic_new.drop([\"survived\", \"name\", \"sex\", \"ticket\", \"cabin\", \"embarked\", \"boat\", \"home.dest\"], axis=1)\n",
    "y = titanic_new[\"survived\"]\n",
    "titanic_new[\"pclass\"] = pd.to_numeric(titanic_new[\"pclass\"], downcast=\"float\")\n",
    "titanic_new[\"age\"] = pd.to_numeric(titanic_new[\"age\"], downcast=\"float\")\n",
    "titanic_new[\"sibsp\"] = pd.to_numeric(titanic_new[\"sibsp\"], downcast=\"float\")\n",
    "titanic_new[\"parch\"] = pd.to_numeric(titanic_new[\"parch\"], downcast=\"float\")\n",
    "titanic_new[\"fare\"] = pd.to_numeric(titanic_new[\"fare\"], downcast=\"float\")\n",
    "titanic_new[\"body\"] = pd.to_numeric(titanic_new[\"body\"], downcast=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "X_train_test, X_validate, y_train_test, y_validate = train_test_split(X, y, train_size=0.9, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_validate_scaled = scaler.transform(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build the model\n",
    "logreg1 = LogisticRegression()\n",
    "logreg1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: \t0.7786\n",
      "Test Accuracy: \t\t0.8038\n",
      "Validation Accuracy: \t0.7414\n"
     ]
    }
   ],
   "source": [
    "#check the performance\n",
    "print(\"Train Accuracy: \\t{:.4f}\".format(logreg1.score(X_train_scaled, y_train)))\n",
    "print(\"Test Accuracy: \\t\\t{:.4f}\".format(logreg1.score(X_test_scaled, y_test)))\n",
    "print(\"Validation Accuracy: \\t{:.4f}\".format(logreg1.score(X_validate_scaled, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is no overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
